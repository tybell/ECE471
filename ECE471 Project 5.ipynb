{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import keras\n",
    "from keras import Model, regularizers\n",
    "from keras.layers import Embedding, Dense, Flatten, Dropout, Input\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "training_data = pd.read_csv('train.csv', names = ['Class', 'Title', 'Description'])\n",
    "test_data = pd.read_csv('test.csv', names = ['Class', 'Title', 'Description'])\n",
    "training_data['Text_data'] = training_data.Title + training_data.Description\n",
    "test_data['Text_data'] = test_data.Title + test_data.Description\n",
    "\n",
    "def create_vocab(text_data):\n",
    "    Vocabulary = []\n",
    "    for text_datum in text_data:\n",
    "        tokens = nltk.word_tokenize(text_datum)\n",
    "        tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "        for token in tokens:\n",
    "            if token in Vocabulary:\n",
    "                continue\n",
    "            else:\n",
    "                Vocabulary.append(token)\n",
    "    return Vocabulary\n",
    "\n",
    "def create_vocab_indices(vocabulary):\n",
    "    vocab_indices = {}\n",
    "    vocab_indices['ZERO_PAD'] = 0\n",
    "    vocab_indices['UNK'] = 1\n",
    "    i = 2\n",
    "    for word in vocabulary:\n",
    "        vocab_indices[word] = i\n",
    "        i += 1\n",
    "    return vocab_indices\n",
    "\n",
    "def vectorize(text_data, vocab_indices):\n",
    "    x = []\n",
    "    for sequence in text_data:\n",
    "        indices = []\n",
    "        tokens = nltk.word_tokenize(sequence)\n",
    "        tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "        for token in tokens:\n",
    "            index = vocab_indices.get(token, 1)\n",
    "            indices.append(index)\n",
    "        x.append(indices)\n",
    "    return x\n",
    "\n",
    "class Data():\n",
    "    def __init__(self):\n",
    "        self.training_data = training_data\n",
    "        self.test_data = test_data\n",
    "        self.training_raw_text = list(self.training_data.Text_data.values)\n",
    "        self.test_raw_text = list(self.test_data.Text_data.values)\n",
    "        self.num_classes = 4\n",
    "        \n",
    "        self.vocab = create_vocab(self.training_raw_text)\n",
    "        self.vocab_indices = create_vocab_indices(self.vocab)\n",
    "        self.vocab_size = len(self.vocab_indices)\n",
    "        \n",
    "        self.training_x = pad_sequences(vectorize(self.training_raw_text, self.vocab_indices), maxlen = 30, value = 0)\n",
    "        self.training_t = keras.utils.to_categorical(np.array(self.training_data.Class.values)-1)\n",
    "        self.test_x = pad_sequences(vectorize(self.test_raw_text, self.vocab_indices), maxlen = 30, value = 0)\n",
    "        self.test_t = keras.utils.to_categorical(np.array(self.test_data.Class.values)-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\belltyle\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 102000 samples, validate on 18000 samples\n",
      "Epoch 1/3\n",
      "102000/102000 [==============================] - 307s 3ms/step - loss: 0.9359 - acc: 0.6108 - val_loss: 0.7414 - val_acc: 0.7506\n",
      "Epoch 2/3\n",
      "102000/102000 [==============================] - 297s 3ms/step - loss: 0.5712 - acc: 0.8470 - val_loss: 0.4856 - val_acc: 0.8677\n",
      "Epoch 3/3\n",
      "102000/102000 [==============================] - 290s 3ms/step - loss: 0.4560 - acc: 0.8841 - val_loss: 0.4544 - val_acc: 0.8753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23c80029eb8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Computational_Graph(inputs):\n",
    "    vocab_size = data.vocab_size\n",
    "    embedding_size = 50\n",
    "    x = Embedding(input_dim = vocab_size, \n",
    "        output_dim = embedding_size, input_length = 30)(inputs)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(units = 64, activation = 'relu', kernel_regularizer = regularizers.l2(0.01))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(units = 32, activation = 'relu', kernel_regularizer = regularizers.l2(0.01))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    predictions = Dense(units = 4, activation = 'softmax')(x)\n",
    "    return predictions\n",
    "inputs = Input(shape = (30,))\n",
    "model = Model(inputs = inputs, output = Computational_Graph(inputs))\n",
    "\n",
    "model.compile(optimizer='Adamax',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "model.fit(data.training_x, data.training_t, \n",
    "    verbose = True, epochs = 3, \n",
    "    validation_split = 0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "120000/120000 [==============================] - 359s 3ms/step - loss: 0.4204 - acc: 0.8923 1s - loss: 0.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23cd3288898>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data.training_x, data.training_t, \n",
    "    verbose = True, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7600/7600 [==============================] - 0s 48us/step\n",
      "Test Error:  0.8835526315789474\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(data.test_x, data.test_t, verbose = True)\n",
    "print('Test Error: ', score[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
